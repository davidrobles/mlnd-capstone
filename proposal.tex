\documentclass{article}

\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{url}
\usepackage{amsmath,bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[round]{natbib}
\usepackage{enumitem,xcolor}
\usepackage[multiple]{footmisc}

\usepackage[
 pdftitle={Capstone Proposal - Udacity Machine Learning Nanodegree},
 pdfsubject={Machine Learning, Reinforcement Learning, Deep Learning, Artificial Intelligence, Games},
 pdfauthor={David Robles},
 pdfpagemode=UseOutlines,
 pdfborder= {0 0 1.0},
 bookmarks,
 bookmarksopen,
 colorlinks=true,
 citecolor=blue,
 linkcolor=blue, %
 linkbordercolor=blue, %
 urlcolor=blue, %
]{hyperref}

\usepackage[labelfont=bf]{caption}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Machine Learning Nanodegree \\ Capstone Proposal}
\author{David A. Robles}
\date{December 13, 2016}
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Domain Background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Reinforcement learning is the area of machine learning concerned with the idea of learning by
interacting with an environment~\citep{Sutton1998RL}. It has a long and notable history in games.
The checkers program written by \citet{Samuel1959Checkers} was the first remarkable application of
temporal difference learning, and also the first significant learning program of any kind. It had
the principles of temporal difference learning decades before it was described and analyzed.
However, it was another game where reinforcement learning reached its first big success, when
\textsc{TD-Gammon}~\citep{Tesauro1995TD} reached world-class gameplay in Backgammon by training a
neural network-based evaluation function through self-play.

Deep Learning~\citep{LeCun2015Nature} is another branch of machine learning that allows
computational models that are composed of multiple processing layers to learn representations of
data with multiple levels of abstraction.  Deep learning techniques have dramatically improved the
state-of-the-art in areas such as speech recognition~\citep{Hinton2012Speech}, image
recognition~\citep{Krizhevsky2012ImageNet} and natural language processing~\citep{Colbert2012}.

Reecently, there have been several breakthroughs when combining reinforcement learning and deep
learning. \cite{Mnih2015AtariNature} used a convolutional neural network trained with a variant of
Q-learning and learned to play Atari 2600 games at a human-level. This year, one of the biggest
challenges for artificial intelligence was solved, when Google's DeepMind~\citep{Silver2016GoNature}
created AlphaGo to beat the world's greatest Go player. AlphaGo used deep neural networks trained by
a combination of supervised learning from human expert games, and reinforcement learning from games
of self-play.

\hspace{-2mm}

\fbox{\begin{minipage}{43em}
In this project, we will use reinforcement learning with deep learning to make an agent learn to
play the game of Connect 4 by playing games against itself.
\end{minipage}}
 
%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\URLticapp}{https://itunes.apple.com/us/app/tic-tac-toe-clash/id1041863034?mt=8}
\newcommand{\URLticplay}{https://play.google.com/store/apps/details?id=com.zerosumstudios.tictactoeclash}

My personal motivation for investigating this area more in depth is that we have plans for
developing mobile games that require strong AI. So far we have developed one mobile game, {Tic Tac
Toe Clash}\footnote{\url{\URLticapp}}\textsuperscript{,}\footnote{\url{\URLticplay}}, but Tic Tac
Toe is such a simple game that does not even require an evaluation function, since is possible to
search the full game tree using Alpha-Beta pruning to find the best possible move. But we are in the
process of developing more games, and some of them will require stronger AI, and hand-coding
different heuristics for each particular game is a hard and tedious task. So, my personal goal is to
use this project as a first step to create a generic reinforcement learning framework that uses deep
learning to learn evaluation functions for abstract strategy games without using any domain
knowledge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statement}
\label{sec:problem-statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% http://www.ceptara.com/blog/how-to-write-problem-statement

Developing strong game-playing programs for classic two-player games (e.g. Chess, Checkers, Go) is
important in two respects: first, for humans that play the games looking for an intellectual
challenge, and second, for AI researchers that use the games as testbeds for artificial
intelligence. In both cases, the task for game programmers and AI researchers of writing strong game
AI is a hard and tedious task that requires hours of trial and error adjustments and human
expertise. Therefore, when a strong game-playing algorithm is created to play a specific game, it is
rarely useful for creating an algorithm to play another game, since the domain knowledge is
non-transferable.

For this reason, there is enormous value in using machine learning to learn to play these games without
using any domain knowledge.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Datasets and Inputs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This project will not use a dataset as part of the training, since an agent will use reinforcement
learning to learn to play Connect 4 by playing games against itself.

However, as part of the testing phase, we will use the \emph{Connect 4 Data
Set}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Connect-4}} that is available from the
UCI Machine Learning Repository~\citep{Hettich1998UCI}. This dataset has a total of 67,557
instances, representing all legal 8-ply positions in \mbox{Connect 4} in which neither player has
won yet, and which the next move is not forced. Each instance is described by 42 features, one for
each space in the $6 \times 7$ board, and can belong to one of the classes $\{\textsc{x},
\textsc{o}, \textsc{b}\}$, where \textsc{x} is the first player, \textsc{o} is the second player,
and \textsc{b} is empty. The outcome class is the game theoretical value for the first player, and
can belong to one of the classes $\{\textsc{Win}, \textsc{Loss}, \textsc{Draw}\}$. In
\hyperref[sec:benchmark]{Section \ref*{sec:benchmark}} we discuss how this dataset will be used as a
benchmark model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solution Statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To tackle the problem described in \hyperref[sec:problem-statement]{Section
\ref*{sec:problem-statement}}, we will use Reinforcement learning with Deep Learning to
automatically learn evaluation functions by playing games against itself. Unlike other approaches
that need a very large dataset, this approach will try to learn to play games without any domain
knowledge (no dataset will be used). This is a promising approach for creating game-playing
algorithms for playing other two-player games of perfect information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Benchmark Model}
\label{sec:benchmark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

    \item \textbf{Random agent}. This benchmark consists in playing against an agent that takes
        uniformly random moves. This is the most basic benchmark, but first we have to be sure that
        our learned evaluation function can play better than a random agent before moving into a
        harder benchmark. Also, this will help us to detect bugs in the code and algorithms: if a
        learned value function does not play significantly better than a random agent, is not
        learning. The idea is to test against this benchmark using Alpha-beta pruning at 1, 2 and
        4-ply search.

    \item \textbf{Connect 4 Data Set}. This dataset will be used as the main benchmark. The learned
        value function will be used to predict the game-theoretic outcomes (win, loss or draw) for
        the first player in the 67,557 instances of the dataset.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation Metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

    \item \textbf{Winning percentage.} This metric consists in playing a high number of games (e.g.
        100,000) against another agent (e.g. a random agent), and calculating the average of games
        won by the agent that uses the learned value function.
        
    \item \textbf{Prediction accuracy.} The learned value function will be used to predict the
        game-theoretic outcomes (win, loss or draw) of the board positions in the Connect 4 Data
        Set. Since this is a multi-class classification problem, the accuracy score will be used to
        evaluate the performance.
        
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project Design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Programming Language and Libraries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

    \item \textbf{Python 2.}

    \item \textbf{scikit-learn}. Open source machine learning library for Python.

    \item \textbf{Keras.} Open source neural network library written in Python. It is capable of
        running on top of either Tensorflow or Theano.

    \item \textbf{TensorFlow.} Open source software libraries for deep learning.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Environments}
%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

    \item \textbf{Tic Tac Toe.} Will be used to verify that the reinforcement learning algorithms
        are working as expected, since it is a simple game where an agent can easily search the full
        game tree to find the optimal actions.

    \item \textbf{Connect 4.} The main environment for which we will learn value functions using
        reinforcement learning with deep neural networks.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Machine Learning Design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

    \item \textbf{Type of training experience:} Games against self

    \item \textbf{Target function:} $Q^\pi : \mathcal{S} \times \mathcal{A} \to \mathbb{R}$, where
        $\mathcal{S}$ is the set of \emph{states} (board positions) and $\mathcal{A}$ is the set of
        \emph{actions} (moves), and $\mathbb{R}$ represents the value of being in a state $s \in
        \mathcal{S}$, applying a action $a \in \mathcal{A}$, and following policy $\pi$ thereafter.

    \item \textbf{Representation of learned function:} Deep neural network

    \item \textbf{Learning algorithm:} Q-learning, a model-free online off-policy algorithm, whose
        main strength is that it is able to compare the expected utility of the available actions
        without requiring a model of the environment:

        \begin{equation}
        Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha [r_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t)]
        \end{equation}


\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Additional Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

    \item \textbf{Alpha-beta Pruning.} The most common game tree search algorithm for two-player
        games of perfect information. It will be used as part of the evaluation of the learned value
        functions.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}
